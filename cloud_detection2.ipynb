{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec54f496d5884874abbf0c5dacad3132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_885fee7daad3480593297b7883e368a0",
              "IPY_MODEL_42b9297f99824dcc9b4722d3985290a9",
              "IPY_MODEL_315a94004be64760af77f7ca3c21ec1c"
            ],
            "layout": "IPY_MODEL_780fca0680ef49bdb0614b297e3a97ce"
          }
        },
        "885fee7daad3480593297b7883e368a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74a230a27fe34c4bb533cf7d97c77168",
            "placeholder": "​",
            "style": "IPY_MODEL_e1d7a04cafc549ebbf622b54a2b39f16",
            "value": "Evaluating Model: 100%"
          }
        },
        "42b9297f99824dcc9b4722d3985290a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bd72de3efe84162a0058ceef8548b57",
            "max": 105,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adee8f20a99a41f683bff40e11b5bb98",
            "value": 105
          }
        },
        "315a94004be64760af77f7ca3c21ec1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36434bcb20ac41e984e178f0e6beaafd",
            "placeholder": "​",
            "style": "IPY_MODEL_57a5792d33474c19ae715fc37a433e90",
            "value": " 105/105 [1:23:33&lt;00:00, 42.34s/it]"
          }
        },
        "780fca0680ef49bdb0614b297e3a97ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74a230a27fe34c4bb533cf7d97c77168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d7a04cafc549ebbf622b54a2b39f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bd72de3efe84162a0058ceef8548b57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adee8f20a99a41f683bff40e11b5bb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36434bcb20ac41e984e178f0e6beaafd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a5792d33474c19ae715fc37a433e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "//content/drive/MyDrive/Colab Notebooks/cloud_detection.ipynb"
      ],
      "metadata": {
        "id": "yYCDexI1T-mn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chk0Hzk0PRVq",
        "outputId": "4fe09a0a-1bff-4ab0-ea87-4cf9bfeeda25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"🖥️  Device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"🚀 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"🔧 CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"📦 Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
        "else:\n",
        "    print(\"⚠️ WARNING: GPU is not enabled!\")\n"
      ],
      "metadata": {
        "id": "XnaNaA14Sf3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5640e04a-9961-4684-ee24-c9526c08ca9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🖥️  Device: cuda\n",
            "🚀 GPU: Tesla T4\n",
            "🔧 CUDA Version: 12.6\n",
            "📦 Memory Allocated: 0.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q albumentations==1.3.0 rasterio opencv-python matplotlib seaborn tqdm"
      ],
      "metadata": {
        "id": "cqcz2039VfmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle API and other libraries needed\n",
        "!pip install -q kaggle torch torchvision albumentations rasterio matplotlib opencv-python tqdm\n",
        "\n",
        "# Make kaggle folder\n",
        "!mkdir -p ~/.kaggle"
      ],
      "metadata": {
        "id": "LmG1tu3_Vfi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install dependencies ---\n",
        "!pip install kagglehub rasterio albumentations==1.3.1 matplotlib scikit-learn torch torchvision tqdm --quiet\n",
        "\n",
        "# --- Imports ---\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "print(\"✅ Libraries installed and imported successfully\")"
      ],
      "metadata": {
        "id": "PSPoJb41Vffy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a34a6ae-378f-4873-9993-2000a937ba7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries installed and imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import kagglehub\n",
        "\n",
        "# # Download the 38-Cloud dataset\n",
        "# path = kagglehub.dataset_download(\"sorour/38cloud-cloud-segmentation-in-satellite-images\")\n",
        "# print(\"✅ Dataset downloaded to:\", path)\n",
        "\n",
        "# # # List a few files\n",
        "# # !ls \"$path\" | head -n 20\n"
      ],
      "metadata": {
        "id": "0spJYschVfc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b86cfec-5f5c-4d49-9ab3-c7cdb1add688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming download from 6592397312 bytes (6464244049 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/sorour/38cloud-cloud-segmentation-in-satellite-images?dataset_version_number=4 (6592397312/13056641361) bytes left.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12.2G/12.2G [04:51<00:00, 22.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset downloaded to: /root/.cache/kagglehub/datasets/sorour/38cloud-cloud-segmentation-in-satellite-images/versions/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drive_destination = \"/content/drive/My Drive/Kaggle_Datasets/38-Cloud\"\n",
        "# os.makedirs(drive_destination, exist_ok=True) # Creates the folder if it doesn't exist\n",
        "\n",
        "# # --- 3. Move the Files ---\n",
        "# print(f\"🚚 Moving files from '{path}' to '{drive_destination}'...\")\n",
        "# !mv \"{path}\"/* \"{drive_destination}\"\n",
        "\n",
        "# print(\"✅ All files have been successfully moved to your Google Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPMCXyNp9Yje",
        "outputId": "1481d019-456d-4a0e-b2e7-4060903efc98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚚 Moving files from '/root/.cache/kagglehub/datasets/sorour/38cloud-cloud-segmentation-in-satellite-images/versions/4' to '/content/drive/My Drive/Kaggle_Datasets/38-Cloud'...\n",
            "✅ All files have been successfully moved to your Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# for root, dirs, files in os.walk(path):\n",
        "#     level = root.replace(path, '').count(os.sep)\n",
        "#     indent = ' ' * 2 * (level)\n",
        "#     print(f\"{indent}{os.path.basename(root)}/\")\n",
        "#     subindent = ' ' * 2 * (level + 1)\n",
        "#     for f in files[:5]:  # show first 5 files per folder\n",
        "#         print(f\"{subindent}{f}\")\n"
      ],
      "metadata": {
        "id": "_TCsa4aZVfZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base path to training data inside downloaded dataset\n",
        "path = \"/content/drive/My Drive/Kaggle_Datasets/38-Cloud\"\n",
        "base_train_path = os.path.join(path, '38-Cloud_training')\n",
        "\n",
        "train_red_path   = os.path.join(base_train_path, 'train_red')\n",
        "train_green_path = os.path.join(base_train_path, 'train_green')\n",
        "train_blue_path  = os.path.join(base_train_path, 'train_blue')\n",
        "train_gt_path    = os.path.join(base_train_path, 'train_gt')  # ground truth masks\n",
        "\n",
        "# Check how many files we have in each band/mask\n",
        "import glob\n",
        "\n",
        "n_red   = len(glob.glob(os.path.join(train_red_path, '*.TIF')))\n",
        "n_green = len(glob.glob(os.path.join(train_green_path, '*.TIF')))\n",
        "n_blue  = len(glob.glob(os.path.join(train_blue_path, '*.TIF')))\n",
        "n_masks = len(glob.glob(os.path.join(train_gt_path, '*.TIF')))\n",
        "\n",
        "print(f\"Red band patches:   {n_red}\")\n",
        "print(f\"Green band patches: {n_green}\")\n",
        "print(f\"Blue band patches:  {n_blue}\")\n",
        "print(f\"Masks (GT):         {n_masks}\")\n"
      ],
      "metadata": {
        "id": "FG9MG1feVfVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea897b6-354e-4056-cc76-3d1ca191e1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red band patches:   8400\n",
            "Green band patches: 8400\n",
            "Blue band patches:  8400\n",
            "Masks (GT):         8400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Pick a random patch from red folder\n",
        "red_files   = sorted(glob.glob(os.path.join(train_red_path, '*.TIF')))\n",
        "green_files = sorted(glob.glob(os.path.join(train_green_path, '*.TIF')))\n",
        "blue_files  = sorted(glob.glob(os.path.join(train_blue_path, '*.TIF')))\n",
        "mask_files  = sorted(glob.glob(os.path.join(train_gt_path, '*.TIF')))\n",
        "\n",
        "idx = random.randint(0, len(red_files)-1)\n",
        "\n",
        "# Read the bands\n",
        "with rasterio.open(red_files[idx]) as src:\n",
        "    red = src.read(1)\n",
        "with rasterio.open(green_files[idx]) as src:\n",
        "    green = src.read(1)\n",
        "with rasterio.open(blue_files[idx]) as src:\n",
        "    blue = src.read(1)\n",
        "\n",
        "# Stack to RGB\n",
        "rgb_img = np.dstack((red, green, blue)).astype(np.float32)\n",
        "rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max()-rgb_img.min())  # normalize to [0,1]\n",
        "\n",
        "# Read mask\n",
        "with rasterio.open(mask_files[idx]) as src:\n",
        "    mask = src.read(1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(rgb_img)\n",
        "plt.title('RGB Image Patch')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.title('Cloud Mask')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XzEiPjyFVfSl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "16e0b33c-5120-4731-ecf6-a8d223a5c673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/rasterio/__init__.py:356: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
            "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
            "/tmp/ipython-input-35776508.py:19: RuntimeWarning: invalid value encountered in divide\n",
            "  rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max()-rgb_img.min())  # normalize to [0,1]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 — PyTorch Dataset Class\n"
      ],
      "metadata": {
        "id": "w9xhnRycYkca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class CloudDataset(Dataset):\n",
        "    def __init__(self, red_files, green_files, blue_files, mask_files, transform=None):\n",
        "        self.red_files = red_files\n",
        "        self.green_files = green_files\n",
        "        self.blue_files = blue_files\n",
        "        self.mask_files = mask_files\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.red_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Read image bands\n",
        "        with rasterio.open(self.red_files[idx]) as src:\n",
        "            red = src.read(1)\n",
        "        with rasterio.open(self.green_files[idx]) as src:\n",
        "            green = src.read(1)\n",
        "        with rasterio.open(self.blue_files[idx]) as src:\n",
        "            blue = src.read(1)\n",
        "\n",
        "        # Read the ground truth mask\n",
        "        with rasterio.open(self.mask_files[idx]) as src:\n",
        "            mask = src.read(1)\n",
        "\n",
        "        # Stack bands into an RGB image and normalize to [0, 1]\n",
        "        img = np.dstack((red, green, blue)).astype(np.float32)\n",
        "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
        "\n",
        "        # ✅ **FIX:** Normalize mask values from [0, 255] to [0, 1]\n",
        "        mask = (mask / 255.0).astype(np.float32)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "        mask = torch.from_numpy(mask).unsqueeze(0)    # (H, W) -> (1, H, W)\n",
        "\n",
        "        return img, mask\n"
      ],
      "metadata": {
        "id": "G6d3w5qWVfPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Split indices for training and validation sets\n",
        "train_idx, val_idx = train_test_split(range(len(red_files)), test_size=0.2, random_state=42)\n",
        "\n",
        "# Create training dataset\n",
        "train_dataset = CloudDataset(\n",
        "    [red_files[i] for i in train_idx],\n",
        "    [green_files[i] for i in train_idx],\n",
        "    [blue_files[i] for i in train_idx],\n",
        "    [mask_files[i] for i in train_idx]\n",
        ")\n",
        "\n",
        "# Create validation dataset\n",
        "val_dataset = CloudDataset(\n",
        "    [red_files[i] for i in val_idx],\n",
        "    [green_files[i] for i in val_idx],\n",
        "    [blue_files[i] for i in val_idx],\n",
        "    [mask_files[i] for i in val_idx]\n",
        ")\n",
        "\n",
        "\n",
        "# ✅ **FIX:** Set num_workers=0 to avoid multiprocessing errors in notebooks\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "id": "90cfn6WoVfMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83be235a-0932-4a20-fb30-708278459693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 420, Validation batches: 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, masks = next(iter(train_loader))\n",
        "print(\"Images shape:\", imgs.shape)   # [B,3,H,W]\n",
        "print(\"Masks shape:\", masks.shape)   # [B,1,H,W]\n",
        "\n",
        "# Plot first sample\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(imgs[0].permute(1,2,0))\n",
        "plt.title('RGB Patch')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(masks[0][0], cmap='gray')\n",
        "plt.title('Mask')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uxqlDVDOVfJa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "de2eccc2-6369-4795-d381-aea870069b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images shape: torch.Size([16, 3, 384, 384])\n",
            "Masks shape: torch.Size([16, 1, 384, 384])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Model (UNet Lite)"
      ],
      "metadata": {
        "id": "EJ9Cj8OrY6qM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Basic UNet block\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=1):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(64, 128))\n",
        "        self.down2 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(128, 256))\n",
        "        self.down3 = nn.Sequential(nn.MaxPool2d(2), DoubleConv(256, 512))\n",
        "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.conv1 = DoubleConv(512, 256)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv2 = DoubleConv(256, 128)\n",
        "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv3 = DoubleConv(128, 64)\n",
        "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x = self.up1(x4)\n",
        "        x = self.conv1(torch.cat([x, x3], dim=1))\n",
        "        x = self.up2(x)\n",
        "        x = self.conv2(torch.cat([x, x2], dim=1))\n",
        "        x = self.up3(x)\n",
        "        x = self.conv3(torch.cat([x, x1], dim=1))\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "# Instantiate model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNet().to(device)\n",
        "print(\"Model ready on\", device)\n"
      ],
      "metadata": {
        "id": "Pqhv8s4zVfGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856d695e-fbb0-4461-ebce-3b7003663e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the loader:\n",
        "for images, masks in train_loader:\n",
        "    print(images.shape, masks.shape)\n",
        "    break  # Only print for the first batch\n"
      ],
      "metadata": {
        "id": "qW0TpfGyVfC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf48b23a-a886-4d8c-d0b7-23f95cda84e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 384, 384]) torch.Size([16, 1, 384, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "from torch.amp import autocast, GradScaler\n",
        "import warnings\n",
        "\n",
        "# Suppress non-critical warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --- DEVICE SETUP ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- INITIALIZE MODEL, LOSS, OPTIMIZER, & SCALER ---\n",
        "# Ensure model is defined and moved to the correct device\n",
        "# model = UNet().to(device) # This should already be defined in a previous cell\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "# Use GradScaler only if a CUDA device is available\n",
        "scaler = GradScaler(device='cuda')\n",
        "\n",
        "# --- METRIC FUNCTIONS (IoU & Dice) ---\n",
        "def compute_metrics(preds, masks, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Computes Dice and IoU scores for a batch of predictions and masks.\n",
        "    \"\"\"\n",
        "    # Sigmoid + threshold\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds = (preds > 0.5).float()\n",
        "    masks = masks.float()\n",
        "\n",
        "    # Flatten\n",
        "    preds = preds.view(preds.size(0), -1)\n",
        "    masks = masks.view(masks.size(0), -1)\n",
        "\n",
        "    # Intersection & sums\n",
        "    intersection = (preds * masks).sum(dim=1)\n",
        "    preds_sum = preds.sum(dim=1)\n",
        "    masks_sum = masks.sum(dim=1)\n",
        "    union = preds_sum + masks_sum - intersection\n",
        "\n",
        "    # Convert smooth to tensor on same device\n",
        "    smooth_tensor = torch.tensor(smooth, device=preds.device)\n",
        "\n",
        "    # Dice & IoU\n",
        "    dice = (2.0 * intersection + smooth_tensor) / (preds_sum + masks_sum + smooth_tensor)\n",
        "    iou = (intersection + smooth_tensor) / (union + smooth_tensor)\n",
        "\n",
        "    return dice.mean().item(), iou.mean().item()\n",
        "\n",
        "\n",
        "\n",
        "# --- TRAINING FUNCTION ---\n",
        "def train_one_epoch(model, loader, optimizer, criterion, scaler, device):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, masks in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Use automatic mixed precision if a scaler is available (on CUDA)\n",
        "        if scaler:\n",
        "            with autocast(device_type='cuda', dtype=torch.float16):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else: # For CPU\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    return avg_loss\n",
        "\n",
        "# --- VALIDATION FUNCTION ---\n",
        "def validate(model, loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Validates the model on the validation dataset.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total_dice = 0.0\n",
        "    total_iou = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "            # Use automatic mixed precision if on CUDA\n",
        "            if torch.cuda.is_available():\n",
        "                with autocast(device_type='cuda', dtype=torch.float16):\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, masks)\n",
        "            else: # For CPU\n",
        "                 outputs = model(images)\n",
        "                 loss = criterion(outputs, masks)\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            dice, iou = compute_metrics(outputs, masks)\n",
        "            total_dice += dice\n",
        "            total_iou += iou\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    avg_dice = total_dice / len(loader)\n",
        "    avg_iou = total_iou / len(loader)\n",
        "    return avg_loss, avg_iou, avg_dice\n",
        "\n",
        "# --- MAIN TRAINING LOOP ---\n",
        "epochs = 10\n",
        "best_val_iou = 0.0\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_iou': [], 'val_dice': []}\n",
        "model_save_path = '/content/drive/MyDrive/colab_models/best_model.pth'\n",
        "\n",
        "\n",
        "# for epoch in range(1, epochs + 1):\n",
        "#     print(f\"\\n📘 Epoch {epoch}/{epochs}\")\n",
        "\n",
        "#     train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler, device)\n",
        "#     history['train_loss'].append(train_loss)\n",
        "#     print(f\"✅ Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "#     val_loss, val_iou, val_dice = validate(model, val_loader, criterion, device)\n",
        "#     history['val_loss'].append(val_loss)\n",
        "#     history['val_iou'].append(val_iou)\n",
        "#     history['val_dice'].append(val_dice)\n",
        "#     print(f\"🔍 Val Loss: {val_loss:.4f} | IoU: {val_iou:.4f} | Dice: {val_dice:.4f}\")\n",
        "\n",
        "#     # Save the model if it has the best validation IoU so far\n",
        "#     if val_iou > best_val_iou:\n",
        "#         best_val_iou = val_iou\n",
        "#         torch.save(model.state_dict(), model_save_path)\n",
        "#         print(f\"💾 Best model saved with IoU: {best_val_iou:.4f}\")\n",
        "\n",
        "#     # Clear CUDA cache to free up memory\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n🏁 Training Complete!\")"
      ],
      "metadata": {
        "id": "bGDQQOWPVe_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c510df92-a9cf-46cb-d39a-fd23e7579832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "🏁 Training Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = torch.tensor([[[[0,1],[1,0]]]], dtype=torch.float)\n",
        "mask = torch.tensor([[[[1,1],[0,0]]]], dtype=torch.float)\n",
        "dice, iou = compute_metrics(pred, mask)\n",
        "print(\"dice = \",dice, \"\\nIOU = \", iou)"
      ],
      "metadata": {
        "id": "BcTViMaTVe80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9383e06-d6b2-4788-f44f-2d37b2d95dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dice =  0.5000001192092896 \n",
            "IOU =  0.3333335518836975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mask min:\", masks.min().item(), \"Mask max:\", masks.max().item())\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(images.to(device))\n",
        "print(\"Output min:\", out.min().item(), \"Output max:\", out.max().item())\n"
      ],
      "metadata": {
        "id": "dbNAVuTNVe52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9b9e87-288f-4993-edaf-3e438822c90e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask min: 0.0 Mask max: 1.0\n",
            "Output min: -6.533498764038086 Output max: 4.180474758148193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def evaluate_all_in_one(\n",
        "    model, loader, device, criterion=None, model_path='/content/drive/MyDrive/colab_models/best_model.pth',\n",
        "    history=None, thresholds=np.linspace(0.1,0.9,9)\n",
        "):\n",
        "    \"\"\"\n",
        "    All-in-one evaluation:\n",
        "    - Loss, Pixel Accuracy\n",
        "    - Global & Mean IoU/Dice\n",
        "    - Precision, Recall, F1\n",
        "    - Confusion Matrix\n",
        "    - IoU/Dice/Loss curves over epochs (if history provided)\n",
        "    - Threshold sweep for IoU\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Load best model weights ---\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        print(f\"✅ Best model weights ('{model_path}') loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"⚠️ '{model_path}' not found. Ensure training was completed.\")\n",
        "        return\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds_logits = []\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    total_loss = 0.0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(loader, desc=\"Evaluating Model\"):\n",
        "            images = images.to(device, dtype=torch.float)\n",
        "            masks = masks.to(device, dtype=torch.float)\n",
        "\n",
        "            outputs = model(images)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            all_preds_logits.append(probs.cpu())\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(masks.cpu())\n",
        "\n",
        "            if criterion is not None:\n",
        "                loss = criterion(outputs, masks)\n",
        "                total_loss += loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "    all_preds_logits = torch.cat(all_preds_logits, dim=0)\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "    all_targets = torch.cat(all_targets, dim=0)\n",
        "\n",
        "    # --- Metrics ---\n",
        "    avg_loss = total_loss / total_batches if total_batches > 0 else None\n",
        "\n",
        "    # Pixel accuracy\n",
        "    correct = (all_preds == all_targets).sum().item()\n",
        "    total = all_targets.numel()\n",
        "    pixel_accuracy = (correct / total) * 100\n",
        "\n",
        "    # Flatten for global metrics\n",
        "    preds_flat = all_preds.view(-1)\n",
        "    targets_flat = all_targets.view(-1)\n",
        "\n",
        "    intersection_global = (preds_flat * targets_flat).sum().item()\n",
        "    union_global = preds_flat.sum().item() + targets_flat.sum().item() - intersection_global\n",
        "    iou_global = (intersection_global + 1e-6) / (union_global + 1e-6)\n",
        "    dice_global = (2. * intersection_global + 1e-6) / \\\n",
        "        (preds_flat.sum().item() + targets_flat.sum().item() + 1e-6)\n",
        "\n",
        "    # Mean IoU/Dice per image\n",
        "    intersection = (all_preds * all_targets).sum(dim=(1,2,3))\n",
        "    union = all_preds.sum(dim=(1,2,3)) + all_targets.sum(dim=(1,2,3)) - intersection\n",
        "    iou_mean = ((intersection + 1e-6) / (union + 1e-6)).mean().item()\n",
        "\n",
        "    dice_mean = ((2 * intersection + 1e-6) /\n",
        "                 (all_preds.sum(dim=(1,2,3)) + all_targets.sum(dim=(1,2,3)) + 1e-6)).mean().item()\n",
        "\n",
        "    # Precision, Recall, F1 (pixel-wise)\n",
        "    tp = (preds_flat * targets_flat).sum().item()\n",
        "    fp = ((preds_flat == 1) & (targets_flat == 0)).sum().item()\n",
        "    fn = ((preds_flat == 0) & (targets_flat == 1)).sum().item()\n",
        "    precision = tp / (tp + fp + 1e-6)\n",
        "    recall = tp / (tp + fn + 1e-6)\n",
        "    f1_score = 2 * precision * recall / (precision + recall + 1e-6)\n",
        "\n",
        "    # --- Confusion Matrix ---\n",
        "    cm = confusion_matrix(targets_flat.numpy(), preds_flat.numpy(), labels=[0,1])\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0,1], yticklabels=[0,1])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix (pixels)')\n",
        "    plt.show()\n",
        "\n",
        "    # --- Threshold sweep IoU ---\n",
        "    iou_vs_threshold = []\n",
        "    for t in thresholds:\n",
        "        preds_t = (all_preds_logits > t).float().view(-1)\n",
        "        inter = (preds_t * targets_flat).sum().item()\n",
        "        uni = preds_t.sum().item() + targets_flat.sum().item() - inter\n",
        "        iou_vs_threshold.append((t, (inter + 1e-6) / (uni + 1e-6)))\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot([x[0] for x in iou_vs_threshold], [x[1] for x in iou_vs_threshold], marker='o')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('IoU')\n",
        "    plt.title('IoU vs Threshold')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # --- History curves if provided ---\n",
        "    if history is not None:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        if 'val_loss' in history and 'train_loss' in history:\n",
        "            plt.plot(history['train_loss'], label='Train Loss')\n",
        "            plt.plot(history['val_loss'], label='Val Loss')\n",
        "        if 'val_iou' in history:\n",
        "            plt.plot(history['val_iou'], label='Val IoU')\n",
        "        if 'val_dice' in history:\n",
        "            plt.plot(history['val_dice'], label='Val Dice')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.title('Training History')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    # --- Final report ---\n",
        "    print(\"\\n--- Final Performance Report ---\")\n",
        "    if avg_loss is not None:\n",
        "        print(f\"📉 Average Loss:          {avg_loss:.4f}\")\n",
        "    print(f\"🎯 Pixel Accuracy:         {pixel_accuracy:.2f}%\")\n",
        "    print(f\"📏 Precision:              {precision:.4f}\")\n",
        "    print(f\"📏 Recall:                 {recall:.4f}\")\n",
        "    print(f\"🎲 F1 Score:               {f1_score:.4f}\")\n",
        "    print(\"\\n📐 IoU Scores:\")\n",
        "    print(f\"    - Global IoU:          {iou_global:.4f}\")\n",
        "    print(f\"    - Mean IoU (per image):{iou_mean:.4f}\")\n",
        "    print(\"\\n🎲 Dice Scores:\")\n",
        "    print(f\"    - Global Dice:         {dice_global:.4f}\")\n",
        "    print(f\"    - Mean Dice (per image):{dice_mean:.4f}\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    return {\n",
        "        'avg_loss': avg_loss,\n",
        "        'pixel_accuracy': pixel_accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score,\n",
        "        'iou_global': iou_global,\n",
        "        'dice_global': dice_global,\n",
        "        'iou_mean': iou_mean,\n",
        "        'dice_mean': dice_mean,\n",
        "        'iou_vs_threshold': iou_vs_threshold\n",
        "    }\n",
        "\n",
        "# --- Usage ---\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "history = None\n",
        "results = evaluate_all_in_one(model, val_loader, device, criterion=criterion, history=history)\n"
      ],
      "metadata": {
        "id": "NBeQMps3Ve20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ec54f496d5884874abbf0c5dacad3132",
            "885fee7daad3480593297b7883e368a0",
            "42b9297f99824dcc9b4722d3985290a9",
            "315a94004be64760af77f7ca3c21ec1c",
            "780fca0680ef49bdb0614b297e3a97ce",
            "74a230a27fe34c4bb533cf7d97c77168",
            "e1d7a04cafc549ebbf622b54a2b39f16",
            "3bd72de3efe84162a0058ceef8548b57",
            "adee8f20a99a41f683bff40e11b5bb98",
            "36434bcb20ac41e984e178f0e6beaafd",
            "57a5792d33474c19ae715fc37a433e90"
          ]
        },
        "outputId": "23400063-d148-40b3-81e6-071acf85bf05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Best model weights ('/content/drive/MyDrive/colab_models/best_model.pth') loaded successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating Model:   0%|          | 0/105 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec54f496d5884874abbf0c5dacad3132"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def visualize_predictions(model, loader, device, num_samples=4):\n",
        "    \"\"\"\n",
        "    Loads the best model and visualizes its predictions on a few random samples.\n",
        "    \"\"\"\n",
        "    # --- 1. Load the best model weights ---\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('/content/drive/MyDrive/colab_models/best_model.pth', map_location=device))\n",
        "        print(\"✅ Best model weights loaded for visualization.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"⚠️ 'best_model.pth' not found. Cannot visualize predictions.\")\n",
        "        return\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # --- 2. Get a batch of data ---\n",
        "    images, masks = next(iter(loader))\n",
        "\n",
        "    # --- 3. Run model inference ---\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images.to(device))\n",
        "        # Convert logits to binary predictions\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "\n",
        "    # Move data to CPU for plotting\n",
        "    images = images.cpu()\n",
        "    masks = masks.cpu()\n",
        "    preds = preds.cpu()\n",
        "\n",
        "    # --- 4. Plot the results ---\n",
        "    plt.figure(figsize=(15, num_samples * 5))\n",
        "\n",
        "    # Get random indices from the batch\n",
        "    indices = random.sample(range(len(images)), min(num_samples, len(images)))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        # Original Image\n",
        "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
        "        # Permute from (C, H, W) to (H, W, C) for displaying\n",
        "        image_to_show = images[idx].permute(1, 2, 0)\n",
        "        plt.imshow(image_to_show)\n",
        "        plt.title(f\"Original Image #{idx}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Ground Truth Mask\n",
        "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
        "        # Squeeze the channel dimension (1, H, W) -> (H, W)\n",
        "        mask_to_show = masks[idx].squeeze()\n",
        "        plt.imshow(mask_to_show, cmap='gray')\n",
        "        plt.title(\"Ground Truth Mask\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Predicted Mask\n",
        "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
        "        pred_to_show = preds[idx].squeeze()\n",
        "        plt.imshow(pred_to_show, cmap='gray')\n",
        "        plt.title(\"Predicted Mask\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- How to use the function ---\n",
        "# Make sure 'model' and 'val_loader' are defined from your previous cells.\n",
        "visualize_predictions(model, val_loader, device)"
      ],
      "metadata": {
        "id": "0BgxeY_uVe0K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "fde512dc-f14e-47c8-c960-557d28ce5fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3985434671.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# --- How to use the function ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Make sure 'model' and 'val_loader' are defined from your previous cells.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mvisualize_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix"
      ],
      "metadata": {
        "id": "4R4-5H1FZjTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gemini\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def generate_detailed_report(model, loader, device):\n",
        "    \"\"\"\n",
        "    Generates a detailed classification report and a confusion matrix heatmap.\n",
        "    \"\"\"\n",
        "    # --- Load the best model weights ---\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('/content/drive/MyDrive/colab_models/best_model.pth', map_location=device))\n",
        "        print(\"✅ Best model weights loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"⚠️ 'best_model.pth' not found.\")\n",
        "        return\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    # --- Get predictions for the entire dataset ---\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(loader, desc=\"Generating Predictions for Report\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_targets.append(masks.cpu())\n",
        "\n",
        "    # Concatenate and flatten all tensors\n",
        "    preds_flat = torch.cat(all_preds, dim=0).view(-1).numpy().astype(int)\n",
        "    targets_flat = torch.cat(all_targets, dim=0).view(-1).numpy().astype(int)\n",
        "\n",
        "    # --- 1. Generate and print the Classification Report ---\n",
        "    print(\"\\n--- Pixel-wise Classification Report ---\")\n",
        "    class_names = ['Not Cloud', 'Cloud']\n",
        "    report = classification_report(targets_flat, preds_flat, target_names=class_names)\n",
        "    print(report)\n",
        "\n",
        "    # --- 2. Generate and plot the Confusion Matrix ---\n",
        "    print(\"\\n--- Confusion Matrix ---\")\n",
        "    cm = confusion_matrix(targets_flat, preds_flat)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# --- How to use ---\n",
        "# Assumes 'model', 'val_loader', and 'device' are defined\n",
        "generate_detailed_report(model, val_loader, device)"
      ],
      "metadata": {
        "id": "UXqPL6rFZlGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def compute_confusion_matrix(preds, masks):\n",
        "    \"\"\"\n",
        "    Compute pixel-level confusion matrix for binary segmentation.\n",
        "    preds: raw model output (logits or probs)\n",
        "    masks: ground truth mask\n",
        "    \"\"\"\n",
        "    # Apply sigmoid + threshold\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds = (preds > 0.5).float()\n",
        "    masks = masks.float()\n",
        "\n",
        "    # Flatten both\n",
        "    preds_flat = preds.view(-1).cpu().numpy()\n",
        "    masks_flat = masks.view(-1).cpu().numpy()\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(masks_flat, preds_flat, labels=[0, 1])\n",
        "    return cm\n",
        "\n",
        "def plot_confusion_matrix(cm):\n",
        "    \"\"\"\n",
        "    Plot a confusion matrix.\n",
        "    \"\"\"\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Background\", \"Cloud\"])\n",
        "    disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "    plt.title(\"Pixel-Level Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage after getting predictions:\n",
        "preds = model(images)\n",
        "cm = compute_confusion_matrix(preds, masks)\n",
        "plot_confusion_matrix(cm)\n"
      ],
      "metadata": {
        "id": "3kJ-RmvWZlCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Map"
      ],
      "metadata": {
        "id": "dDhawdfSZsYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gemini\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def visualize_error_map(model, loader, device, num_samples=3):\n",
        "    \"\"\"\n",
        "    Visualizes the model's errors on random samples.\n",
        "    \"\"\"\n",
        "    # --- Load the best model weights ---\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('/content/drive/MyDrive/colab_models/best_model.pth', map_location=device))\n",
        "        print(\"✅ Best model weights loaded successfully for error mapping.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"⚠️ 'best_model.pth' not found.\")\n",
        "        return\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    images, masks = next(iter(loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images.to(device))\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).cpu().float()\n",
        "\n",
        "    images = images.cpu()\n",
        "    masks = masks.cpu()\n",
        "\n",
        "    indices = random.sample(range(len(images)), min(num_samples, len(images)))\n",
        "\n",
        "    for idx in indices:\n",
        "        pred_mask = preds[idx].squeeze()\n",
        "        true_mask = masks[idx].squeeze()\n",
        "\n",
        "        # Create the error map\n",
        "        # True Negative (TN): Correct Background (pred=0, true=0) -> black\n",
        "        # True Positive (TP): Correct Cloud (pred=1, true=1) -> white\n",
        "        # False Positive (FP): Wrongly Predicted Cloud (pred=1, true=0) -> red\n",
        "        # False Negative (FN): Missed Cloud (pred=0, true=1) -> blue\n",
        "\n",
        "        error_map = torch.zeros(true_mask.shape[0], true_mask.shape[1], 3)\n",
        "\n",
        "        # TP (white)\n",
        "        error_map[(pred_mask == 1) & (true_mask == 1)] = torch.tensor([1, 1, 1])\n",
        "        # FP (red)\n",
        "        error_map[(pred_mask == 1) & (true_mask == 0)] = torch.tensor([1, 0, 0])\n",
        "        # FN (blue)\n",
        "        error_map[(pred_mask == 0) & (true_mask == 1)] = torch.tensor([0, 0, 1])\n",
        "\n",
        "        # Plotting\n",
        "        plt.figure(figsize=(18, 5))\n",
        "\n",
        "        plt.subplot(1, 4, 1)\n",
        "        plt.imshow(images[idx].permute(1, 2, 0))\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 2)\n",
        "        plt.imshow(true_mask, cmap='gray')\n",
        "        plt.title(\"Ground Truth\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 3)\n",
        "        plt.imshow(pred_mask, cmap='gray')\n",
        "        plt.title(\"Model Prediction\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 4, 4)\n",
        "        plt.imshow(error_map)\n",
        "        plt.title(\"Error Map (Red=FP, Blue=FN)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# --- How to use ---\n",
        "visualize_error_map(model, val_loader, device)"
      ],
      "metadata": {
        "id": "elEcJTI_ZtC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_error_map(model, val_loader, device, num_samples=3):\n",
        "    \"\"\"\n",
        "    Visualize prediction vs ground truth with error map.\n",
        "    Green = TP, Red = FN, Blue = FP, Gray = TN\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    data_list = list(val_loader)\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # --- pick random batch and sample ---\n",
        "        images, masks = random.choice(data_list)\n",
        "        idx = random.randint(0, images.shape[0]-1)\n",
        "        image = images[idx:idx+1].to(device)\n",
        "        mask = masks[idx:idx+1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            pred = (torch.sigmoid(output) > 0.5).float()\n",
        "\n",
        "        # Move to CPU for visualization\n",
        "        img_np = image[0].cpu().permute(1,2,0).numpy()\n",
        "        mask_np = mask[0,0].cpu().numpy()\n",
        "        pred_np = pred[0,0].cpu().numpy()\n",
        "\n",
        "        # --- Build error map ---\n",
        "        tp = (pred_np == 1) & (mask_np == 1)  # True Positive\n",
        "        fn = (pred_np == 0) & (mask_np == 1)  # False Negative\n",
        "        fp = (pred_np == 1) & (mask_np == 0)  # False Positive\n",
        "        tn = (pred_np == 0) & (mask_np == 0)  # True Negative\n",
        "\n",
        "        error_map = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
        "        error_map[tp] = [0, 255, 0]      # Green for TP\n",
        "        error_map[fn] = [255, 0, 0]      # Red for FN\n",
        "        error_map[fp] = [0, 0, 255]      # Blue for FP\n",
        "        error_map[tn] = [128, 128, 128]  # Gray for TN\n",
        "\n",
        "        # --- Plotting ---\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(18,6))\n",
        "        axs[0].imshow(img_np)\n",
        "        axs[0].set_title(\"Original Image\")\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        axs[1].imshow(mask_np, cmap='gray')\n",
        "        axs[1].set_title(\"Ground Truth\")\n",
        "        axs[1].axis('off')\n",
        "\n",
        "        axs[2].imshow(pred_np, cmap='gray')\n",
        "        axs[2].set_title(\"Predicted Mask\")\n",
        "        axs[2].axis('off')\n",
        "\n",
        "        axs[3].imshow(error_map)\n",
        "        axs[3].set_title(\"Error Map\\n(Green=TP, Red=FN, Blue=FP, Gray=TN)\")\n",
        "        axs[3].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# --- How to call it ---\n",
        "plot_error_map(model, val_loader, device, num_samples=3)\n"
      ],
      "metadata": {
        "id": "HZzmyuKTZk7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overlays on images\n"
      ],
      "metadata": {
        "id": "nu03p8exf6rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Setup output folder ---\n",
        "overlay_dir = '/content/drive/MyDrive/colab_models/cloud_detection_overlays'\n",
        "os.makedirs(overlay_dir, exist_ok=True)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"🔄 Generating overlays and saving to Drive...\")\n",
        "with torch.no_grad():\n",
        "    for idx, (images, masks) in enumerate(val_loader):\n",
        "        images = images.to(device, dtype=torch.float)\n",
        "        masks = masks.to(device, dtype=torch.float)\n",
        "        outputs = model(images)\n",
        "        preds = (torch.sigmoid(outputs) > 0.5).float().cpu()\n",
        "        masks = masks.cpu()\n",
        "        images = images.cpu()\n",
        "\n",
        "        for j in range(images.size(0)):\n",
        "            img_np = images[j].permute(1,2,0).numpy()\n",
        "            img_np = (img_np - img_np.min())/(img_np.max()-img_np.min())  # normalize for display\n",
        "\n",
        "            fig, ax = plt.subplots(1,3,figsize=(12,4))\n",
        "            ax[0].imshow(img_np)\n",
        "            ax[0].set_title('Original')\n",
        "            ax[1].imshow(masks[j,0], cmap='gray')\n",
        "            ax[1].set_title('Ground Truth')\n",
        "            ax[2].imshow(img_np)\n",
        "            ax[2].imshow(preds[j,0], cmap='jet', alpha=0.5)\n",
        "            ax[2].set_title('Prediction Overlay')\n",
        "            for a in ax: a.axis('off')\n",
        "\n",
        "            overlay_path = os.path.join(overlay_dir, f'overlay_{idx}_{j}.png')\n",
        "            plt.savefig(overlay_path, bbox_inches='tight', dpi=150)\n",
        "            plt.close()\n",
        "\n",
        "print(f\"✅ Overlays saved in: {overlay_dir}\")\n"
      ],
      "metadata": {
        "id": "8pZLmGeeZk0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWBc95FfZkpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lc3BKSAYVeKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}